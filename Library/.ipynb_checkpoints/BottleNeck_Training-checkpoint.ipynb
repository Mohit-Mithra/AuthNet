{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape\n",
    "from keras.layers import Convolution1D, Convolution2D, ZeroPadding2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_model=load_model('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Utilities\\\\VGGFace.h5') #load the pre-trained VGGface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(featmodel, crpimg, transform=False): \n",
    "    \"\"\"\n",
    "    Funtion that extracts the face features into numpy arrays\n",
    "    \n",
    "    Parameters: \n",
    "    featmodel: model used for prediction. VGGFace in this case.\n",
    "    crpimg: variable that stores the images and correct RGB values.\n",
    "    \n",
    "    Returns: \n",
    "    output : features of the image passed\n",
    "    \"\"\"\n",
    "    # transform=True seems more robust but I think the RGB channels are not in right order\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:,:,0] -= 129.1863\n",
    "        imarr[:,:,1] -= 104.7624\n",
    "        imarr[:,:,2] -= 93.5940        \n",
    "        aux = copy.copy(imarr)\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "    output= featmodel.predict(imarr)[0,:] #stores predictions\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#timestep: variable that specifies the required timestep\n",
    "def feature_extractor(inpDir,person,timestep,sample): \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that accesses the required photo directories in sequence and extract face features\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: String that stores path of photos directory\n",
    "    person: variable that stores the person number **not a string value**\n",
    "    sample: variable that stores the sample number(sample is the word which is spoken) **not a string value**\n",
    "    \n",
    "    Returns:\n",
    "    feature_sequence_reshaped: Features of the person at the specified timestep across all utternaces of saying the particular word \n",
    "    \n",
    "    \"\"\"\n",
    "    #if ex_type == True:\n",
    "        #Y_train_labels=np.ones((5,2622)) \n",
    "    #else:\n",
    "        #Y_train_labels=np.zeros((5,2622))  \n",
    "        \n",
    "      \n",
    "    sample_space=[] #for appending different utterances\n",
    "    feature_sequence = [] # for appending different timesteps\n",
    "    #input dataset most likely to be People_cerebrus/photos\n",
    "    linpDir = os.listdir(inpDir) #list all directories in dataset\n",
    "    personStr= linpDir[person]\n",
    "    sampleFolder = '%s\\\\%s' % (inpDir,personStr) #opening sample folder\n",
    "    lsampleFolder = os.listdir(sampleFolder)\n",
    "    i = 0\n",
    "    utterFolder = '%s\\\\%s' % (sampleFolder,lsampleFolder[sample])#opening utterance folders\n",
    "    lutterFolder = os.listdir(utterFolder)\n",
    "    for utterances in lutterFolder:\n",
    "        utterNumber= '%s\\\\%s' % (utterFolder,utterances)\n",
    "        lutterNumber= os.listdir(utterNumber)\n",
    "        frame = lutterNumber[timestep] #accessing images of required timestep\n",
    "        i = i + 1\n",
    "        image= \"%s\\\\%s\" % (utterNumber,frame) \n",
    "        im=Image.open(image)\n",
    "        im = im.resize((224,224)) #resize required to pass through VGGFace\n",
    "        feature_vector = features(trial_model,im, transform=True).reshape((1,1, 1, 2622))\n",
    "        if i==1 :\n",
    "            feature_sequence=feature_vector #done because of need of same dimensions for concatenation\n",
    "        else:\n",
    "            feature_sequence = np.concatenate((feature_sequence,feature_vector),axis=0)\n",
    "    feature_sequence_reshaped = feature_sequence[:,0,:,:] #removing unnecessary extra dimensions\n",
    "    return feature_sequence_reshaped\n",
    "\n",
    "    \n",
    "    #WB_CNN= Sequential()\n",
    "    #WB_CNN.add(Convolution2D(1024, kernel_size=(1,1), activation= 'relu', input_shape=(1,1,2622)))\n",
    "    #WB_CNN.add( Dropout(0.2) )\n",
    "    #WB_CNN.add(Convolution2D(1024, kernel_size=(1,1), activation='relu',input_shape=(1,1,2622)))\n",
    "    #WB_CNN.add( Dropout(0.2) )\n",
    "    #WB_CNN.add(Convolution2D(512, kernel_size=(1,1), activation='relu'))\n",
    "    #WB_CNN.add(Flatten())   \n",
    "\n",
    "    #WB_CNN.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    #WB_CNN.fit(feature_sequence,Y_train_labels,epochs=20,batch_size=2)\n",
    "    #WB_CNN.save('Bottle_Neck.h5')\n",
    "    #CNN_out = WB_CNN.predict(feature_sequence)\n",
    "    #print(CNN_out.shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extractor(inpDir,person,sample):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that extracts face features of all images across all timesteps for one word.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: String that stores path of photos directory\n",
    "    person: variable that stores the person number **not a string value**\n",
    "    sample: variable that stores the sample number(sample is the word which is spoken) **not a string value**\n",
    "    \n",
    "    Returns:\n",
    "    CNN_out_total: Features of all utterances of the sample/word across all the timesteps\n",
    "    \n",
    "    \"\"\"\n",
    "    timesteps=20 #optimum number of timesteps.Change can be made.\n",
    "    condition_count=0 #\n",
    "    for i in range(timesteps):\n",
    "        CNN_out=feature_extractor(inpDir,person,i,sample)#calling feature_extractor()\n",
    "        condition_count = condition_count + 1\n",
    "        if condition_count==1 :\n",
    "            CNN_out_total= CNN_out # extra condtion is added to maintain same dimensions\n",
    "        else :\n",
    "            CNN_out_total= np.concatenate((CNN_out_total,CNN_out))\n",
    "    return CNN_out_total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-af3e71cf0dad>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-af3e71cf0dad>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    CNN_out_total= word_extractor(InpDir,person)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def person_extractor(InpDir,person):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that extracts face features of all images for the person.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: variable that stores path of photos directory\n",
    "    person: variable that stores the person number **not a string value**\n",
    "    \n",
    "    Returns:\n",
    "    p_dataset: list that has the features extracted for all the videos of the person\n",
    "    \n",
    "    \"\"\"\n",
    "    p_dataset=[] #array to store values\n",
    "    condition_count=0\n",
    "    no_of_words=3 #one positive and 2 negatives. Can be changed.\n",
    "    for i in range(no_of_words):  #Iterate through all the words spoken by the person\n",
    "        condition_count = condition_count +1\n",
    "        CNN_out_total= word_extractor(InpDir,person)\n",
    "        if count==1 :\n",
    "            p_dataset = CNN_out_total #done because of need of same dimensions for concatenation\n",
    "        else:\n",
    "            p_dataset = np.concatenate((p_dataset,CNN_out_total),axis=0)\n",
    "    return p_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_LSTM(sample_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to reshape the face features array into a LSTM type input\n",
    "\n",
    "    Parameters:\n",
    "    sample_mat: list that has the dataset of extracted features for 1 person\n",
    "    \n",
    "    Returns:\n",
    "    LSTM_input: list that has the image features in the appropriate sequence so that it can be reshaped to LSTM input format\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_vector=[]\n",
    "    feature_mat=[]\n",
    "    \n",
    "    word_no = 3\n",
    "    utterance_no = 5\n",
    "    \n",
    "    count_word = 0\n",
    "    for word in range(word_no):  #Go through the number of words\n",
    "        count_i=0\n",
    "        for utterance in range(utterance_no):  #Go through the number of utterances\n",
    "            count_j=0\n",
    "            #Take 100 images/features_sequences(each word has 5 utterances * 20 timesteps = 100 images/features_sequences)\n",
    "            for timestep in range((20*utterance_no)*word,(20*utterance_no)*(word+1),utterance_no): \n",
    "                #The images go through each utterance of 1 timestep and then go to the next timestep and repeat\n",
    "                #So, we take every fifth image and concatenate(which gives utterance*20/utterance =  20 images i.e all the timesteps of 1 utternace) \n",
    "                 \n",
    "                timestep = timestep + utterance\n",
    "                sample_mat_j=sample_mat[timestep]\n",
    "                sample_mat_j=sample_mat_j.reshape((1,2622))\n",
    "                if count_j==0:\n",
    "                    feature_vector=sample_mat_j\n",
    "                else:\n",
    "                    feature_vector=np.concatenate((feature_vector,sample_mat_j))\n",
    "                count_j = count_j + 1\n",
    "\n",
    "            if count_i==0:\n",
    "                feature_mat=feature_vector\n",
    "            else:\n",
    "                feature_mat= np.concatenate((feature_mat,feature_vector))\n",
    "                #This has the features for each timestep for 1 utterance. This is repeated for all utternaces in the outer loop\n",
    "            count_i= count_i + 1\n",
    "            print(feature_mat.shape)\n",
    "            \n",
    "        if count_word==0:\n",
    "            LSTM_input=feature_mat\n",
    "        else:\n",
    "            LSTM_input= np.concatenate((LSTM_input,feature_mat))\n",
    "            #This has all the features for 1 word. This is repeated for each word in the outer loop\n",
    "        count_word= count_word + 1\n",
    "                \n",
    "    print(LSTM_input.shape)\n",
    "  \n",
    "    return LSTM_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleNeck(InpDir):\n",
    "    \"\"\"\n",
    "    Function that returns the dataset for training the model.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: variable that stores path of photos directory\n",
    "    \n",
    "    Returns:\n",
    "    p_total_dataset: Dataset that has the features of all the videos of all the people\n",
    "    \"\"\"\n",
    "    \n",
    "    p_dataset=person_extractor(InpDir,0) #could be iterated across multiple people and later concatenated\n",
    "    p_dataset_LSTM=reshape_LSTM(p_dataset)\n",
    "    print('person done')\n",
    "    return p_dataset_LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
