{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape\n",
    "from keras.layers import Convolution1D, Convolution2D, ZeroPadding2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_model=load_model('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Utilities\\\\VGGFace.h5')\n",
    "WB_CNN = load_model('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Library\\\\Bottle_Neck.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(featmodel, crpimg, transform=False): #extract face feature vector\n",
    "    \n",
    "    # transform=True seems more robust but I think the RGB channels are not in right order\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:,:,0] -= 129.1863\n",
    "        imarr[:,:,1] -= 104.7624\n",
    "        imarr[:,:,2] -= 93.5940        \n",
    "        aux = copy.copy(imarr)\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "    output= featmodel.predict(imarr)[0,:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inpDir,person,timestep,sample,ex_type=True): #access directory and extract face vector\n",
    "    \"\"\"\n",
    "    Funtion that extracts the face features into numpy arrays\n",
    "    \n",
    "    Parameters: \n",
    "    featmodel: model used for prediction. VGGFace in this case.\n",
    "    crpimg: variable that stores the images and correct RGB values.\n",
    "    \n",
    "    Returns: \n",
    "    output : features of the image passed\n",
    "    \"\"\"    \n",
    "    \n",
    "     \n",
    "    if ex_type == True:\n",
    "        Y_train_labels=np.ones((1,2622)) #*******\n",
    "    else:\n",
    "        Y_train_labels=np.zeros((1,2622))  #remove after training stage is completed  **********\n",
    "        \n",
    "     \n",
    "    sample_space=[] #for appending different utterances\n",
    "    feature_sequence = [] # for appending different timesteps\n",
    "     #input dataset most likely to be People_cerebrus/photos\n",
    "    linpDir = os.listdir(inpDir) #list all directories in dataset\n",
    "    #print(linpDir)\n",
    "    personStr= linpDir[person]\n",
    "    sampleFolder = '%s\\\\%s' % (inpDir,personStr) #opening sample folder\n",
    "    lsampleFolder = os.listdir(sampleFolder)\n",
    "    print(lsampleFolder)\n",
    "    sample= lsampleFolder[sample]\n",
    "    \n",
    "    utterFolder = '%s\\\\%s' % (sampleFolder,sample)  #opening utterance folder\n",
    "    i = 0\n",
    "    lutterFolder = os.listdir(utterFolder) \n",
    "    \n",
    "    for utterances in lutterFolder:\n",
    "        print(utterances)\n",
    "        utterNumber= '%s\\\\%s' % (utterFolder,utterances)\n",
    "        lutterNumber= os.listdir(utterNumber)\n",
    "\n",
    "        frame = lutterNumber[timestep] #accessing images of required timestep\n",
    "        i = i + 1\n",
    "        image= \"%s\\\\%s\" % (utterNumber,frame) \n",
    "        im=Image.open(image)\n",
    "        im = im.resize((224,224))\n",
    "        feature_vector = features(trial_model,im, transform=True).reshape((1,1, 1, 2622))  \n",
    "        if i==1 :\n",
    "            feature_sequence=feature_vector #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            feature_sequence = np.concatenate((feature_sequence,feature_vector),axis=0)\n",
    "\n",
    "    print(\"Extraction completed %d\"%i)\n",
    "    \n",
    "    feature_sequence1 = feature_sequence[:,0,:,:]\n",
    "    return feature_sequence1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extractor(inpDir,person,sample,ex_type):\n",
    "        \n",
    "    \"\"\"\n",
    "    Function that extracts face features of all images across all timesteps for one word.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: String that stores path of photos directory\n",
    "    person: variable that stores the person number **not a string value**\n",
    "    sample: variable that stores the sample number(sample is the word which is spoken) **not a string value**\n",
    "    \n",
    "    Returns:\n",
    "    CNN_out_total: Features of all utterances of the sample/word across all the timesteps\n",
    "    \n",
    "    \"\"\"\n",
    "    timesteps=20 \n",
    "    count=0\n",
    "    for i in range(timesteps):\n",
    "        CNN_out=feature_extractor(inpDir,person,i,sample,ex_type)\n",
    "        count = count + 1\n",
    "        if count==1 :\n",
    "            CNN_out_total= CNN_out #maintaining same dimensions\n",
    "        else :\n",
    "            CNN_out_total= np.concatenate((CNN_out_total,CNN_out))\n",
    "        print(str(i) + 'timestep')\n",
    "        \n",
    "    print('Concatenation completed')\n",
    "    return CNN_out_total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_extractor(InpDir,person):\n",
    "        \n",
    "    \"\"\"\n",
    "    Function that extracts face features of all images for the person.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: variable that stores path of photos directory\n",
    "    person: variable that stores the person number **not a string value**\n",
    "    \n",
    "    Returns:\n",
    "    p_dataset: list that has the features extracted for all the videos of the person\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    p_dataset=[]\n",
    "    no_of_words=3\n",
    "    count=0\n",
    "    for i in range(no_of_words): #Iterate through all the words spoken by the person\n",
    "        count = count +1\n",
    "        if person==0 and i==0:\n",
    "            CNN_out_total= word_extractor(InpDir,person,i,True)\n",
    "        else:\n",
    "            CNN_out_total= word_extractor(InpDir,person,i,False)\n",
    "        if count==1 :\n",
    "            p_dataset = CNN_out_total #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            p_dataset = np.concatenate((p_dataset,CNN_out_total),axis=0)\n",
    "        print(str(i) + 'word')\n",
    "        print('Printing Dataset')\n",
    "        print(p_dataset)\n",
    "    print('----------------')\n",
    "    print(count)\n",
    "    print(p_dataset.shape)\n",
    "    return p_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_LSTM(sample_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to reshape the face features array into a LSTM type input\n",
    "\n",
    "    Parameters:\n",
    "    sample_mat: list that has the dataset of extracted features for 1 person\n",
    "    \n",
    "    Returns:\n",
    "    LSTM_input: list that has the image features in the appropriate sequence so that it can be reshaped to LSTM input format\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_vector=[]\n",
    "    feature_mat=[]\n",
    "    \n",
    "    word_no = 3\n",
    "    utterance_no = 1\n",
    "    \n",
    "    count_word = 0\n",
    "    for word in range(word_no):  #Go through the number of words\n",
    "        count_i=0\n",
    "        for utterance in range(utterance_no):  #Go through the number of utterances\n",
    "            count_j=0\n",
    "            #Take 100 images/features_sequences(each word has 5 utterances * 20 timesteps = 100 images/features_sequences)\n",
    "            for timestep in range((20*utterance_no)*word,(20*utterance_no)*(word+1),utterance_no): \n",
    "                #The images go through each utterance of 1 timestep and then go to the next timestep and repeat\n",
    "                #So, we take every fifth image and concatenate(which gives utterance*20/utterance =  20 images i.e all the timesteps of 1 utternace) \n",
    "                 \n",
    "                timestep = timestep + utterance\n",
    "                sample_mat_j=sample_mat[timestep]\n",
    "                sample_mat_j=sample_mat_j.reshape((1,2622))\n",
    "                if count_j==0:\n",
    "                    feature_vector=sample_mat_j\n",
    "                else:\n",
    "                    feature_vector=np.concatenate((feature_vector,sample_mat_j))\n",
    "                count_j = count_j + 1\n",
    "\n",
    "            if count_i==0:\n",
    "                feature_mat=feature_vector\n",
    "            else:\n",
    "                feature_mat= np.concatenate((feature_mat,feature_vector))\n",
    "                #This has the features for each timestep for 1 utterance. This is repeated for all utternaces in the outer loop\n",
    "            count_i= count_i + 1\n",
    "            print(feature_mat.shape)\n",
    "            \n",
    "        if count_word==0:\n",
    "            LSTM_input=feature_mat\n",
    "        else:\n",
    "            LSTM_input= np.concatenate((LSTM_input,feature_mat))\n",
    "            #This has all the features for 1 word. This is repeated for each word in the outer loop\n",
    "        count_word= count_word + 1\n",
    "                \n",
    "    print(LSTM_input.shape)\n",
    "  \n",
    "    return LSTM_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleNeck(InpDir):\n",
    "    \"\"\"\n",
    "    Function that returns the dataset for training the model.\n",
    "    \n",
    "    Parameters:\n",
    "    inpDir: variable that stores path of photos directory\n",
    "    \n",
    "    Returns:\n",
    "    p_total_dataset: Dataset that has the features of all the videos of all the people\n",
    "    \"\"\"\n",
    "    \n",
    "    p_dataset=person_extractor(InpDir,0)\n",
    "    p_dataset_LSTM=reshape_LSTM(p_dataset)\n",
    "    print('person done')\n",
    "    return p_dataset_LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
