{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape\n",
    "from keras.layers import Convolution1D, Convolution2D, ZeroPadding2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Om\\Anaconda3\\envs\\opencv4\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "trial_model=load_model('VGGFace.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(featmodel, crpimg, transform=False): #extract face feature vector\n",
    "    \n",
    "    # transform=True seems more robust but I think the RGB channels are not in right order\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:,:,0] -= 129.1863\n",
    "        imarr[:,:,1] -= 104.7624\n",
    "        imarr[:,:,2] -= 93.5940        \n",
    "        aux = copy.copy(imarr)\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "    output= featmodel.predict(imarr)[0,:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inpDir,person,timestep,sample,ex_type=True): #access directory and extract face vector\n",
    "    \n",
    "    \n",
    "    if ex_type == True:\n",
    "        Y_train_labels=np.ones((5,512))\n",
    "    else:\n",
    "        Y_train_labels=np.zeros((5,512))  #remove after training stage is completed\n",
    "        \n",
    "      \n",
    "    sample_space=[] #for appending different utterances\n",
    "    feature_sequence = [] # for appending different timesteps\n",
    "    inpDir='inpDir' #input dataset most likely to be People_cerebrus/photos\n",
    "    linpDir = os.listdir(inpDir) #list all directories in dataset\n",
    "    personStr= linpDir[person]\n",
    "    sampleFolder = '%s\\\\%s' % (inpDir,personStr) #opening sample folder\n",
    "    lsampleFolder = os.listdir(sampleFolder)\n",
    "    for sample in lsampleFolder:\n",
    "        utterFolder = '%s\\\\%s' % (sampleFolder,sample)\n",
    "        lutterFolder = os.listdir(utterFolder)\n",
    "        for utterances in lutterFolder:\n",
    "            utterNumber= '%s\\\\%s' % (utterFolder,utterances)\n",
    "            lutterNumber= os.listdir(utterNumber)\n",
    "            \n",
    "            frame = lutterNumber[timestep]\n",
    "            i += 1\n",
    "            image= \"%s\\\\%s\" % (colourFolder,frame) \n",
    "            im=Image.open(image)\n",
    "            im = im.resize((224,224))\n",
    "            feature_vector = features(trial_model,im, transform=True).reshape((1,1, 1, 2622))\n",
    "            if i==1 :\n",
    "                feature_sequence=feature_vector #done because of need of same dim. for concatenation\n",
    "            else:\n",
    "                feature_sequence = np.concatenate((feature_sequence,feature_vector),axis=0)\n",
    "\n",
    "    print(\"Extraction completed %d\"%i)\n",
    "    \n",
    "    WB_CNN= Sequential()\n",
    "    WB_CNN.add(Convolution2D(1024, kernel_size=(1,1), activation= 'relu', input_shape=(1,1,2622)))\n",
    "    WB_CNN.add( Dropout(0.2) )\n",
    "    WB_CNN.add(Convolution2D(1024, kernel_size=(1,1), activation='relu',input_shape=(1,1,2622)))\n",
    "    WB_CNN.add( Dropout(0.2) )\n",
    "    WB_CNN.add(Convolution2D(512, kernel_size=(1,1), activation='relu'))\n",
    "    WB_CNN.add(Flatten())   \n",
    "\n",
    "    WB_CNN.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    WB_CNN.fit(feature_sequence,Y_train_labels,epochs=20,batch_size=2)\n",
    "    CNN_out = WB_CNN.predict(feature_sequence)\n",
    "    \n",
    "    print(\"Prediction done\")\n",
    "    print(CNN_out.shape)\n",
    "    return CNN_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_to_feat(inpDir,person,sample,ex_type):\n",
    "    timesteps=20\n",
    "    count=0\n",
    "    for i in range(timesteps):\n",
    "        CNN_out=feature_extractor(inpDir,personID,wordID,i,ex_type)\n",
    "        count = count + 1\n",
    "        if count==1 :\n",
    "            CNN_out_total= CNN_out #maintaining same dimensions\n",
    "        else :\n",
    "            CNN_out_total= np.concatenate((CNN_out_total,CNN_out))\n",
    "        print(str(i) + 'timestep')\n",
    "    \n",
    "    print('Concatenation completed')\n",
    "    return CNN_out_total      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_LSTM(sample_mat):\n",
    "    feature_vector=[]\n",
    "    feature_mat=[]\n",
    "    count_i=0\n",
    "    for i in range(10):\n",
    "        count_j=0\n",
    "        for j in range(0,200,10):\n",
    "            j=j+i\n",
    "            sample_mat_j=sample_mat[j]\n",
    "            sample_mat_j=sample_mat_j.reshape((1,512))\n",
    "            if count_j==0:\n",
    "                feature_vector=sample_mat_j\n",
    "            else:\n",
    "                feature_vector=np.concatenate((feature_vector,sample_mat_j))\n",
    "            count_j = count_j + 1\n",
    "        \n",
    "        if count_i==0:\n",
    "            feature_mat=feature_vector\n",
    "        else:\n",
    "            feature_mat= np.concatenate((feature_mat,feature_vector))\n",
    "        count_i= count_i + 1\n",
    "    LSTM_input= feature_mat\n",
    "    return LSTM_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleNeck(InpDir,person):\n",
    "    p_dataset=[]\n",
    "    count=0\n",
    "    for i in range(5):\n",
    "        count = count +1\n",
    "        if personID==0 and i==0:\n",
    "            CNN_out_total= per_to_feat(InpDir,person,i,True)\n",
    "        else:\n",
    "            CNN_out_total= per_to_feat(InpDir,person,i,False)\n",
    "        if count==1 :\n",
    "            p_dataset = CNN_out_total #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            p_dataset = np.concatenate((p_dataset,CNN_out_total),axis=0)\n",
    "        print(str(i) + 'word')\n",
    "    print('----------------')\n",
    "    print(count)\n",
    "    print(p_dataset.shape)\n",
    "    return p_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def BottleNeck(InpDir,no_of_people):\n",
    "    fcount=0\n",
    "    for i in range(people):\n",
    "        fcount= fcount+1\n",
    "        p_dataset=person_extractor(i)\n",
    "        p_dataset_LSTM=reshape_LSTM(p_dataset)\n",
    "        if fcount==1:\n",
    "            p_total_dataset=p_dataset_LSTM\n",
    "        else:\n",
    "            p_total_dataset=np.concatenate((p_total_dataset,p_dataset_LSTM),axis=0)\n",
    "        \n",
    "        print('person done')\n",
    "    save('test_dataset.npy',p_total_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
