{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape\n",
    "from keras.layers import Convolution1D, Convolution2D, ZeroPadding2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_model=load_model('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Utilities\\\\VGGFace.h5')\n",
    "WB_CNN = load_model('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Library\\\\Bottle_Neck.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(featmodel, crpimg, transform=False): #extract face feature vector\n",
    "    \n",
    "    # transform=True seems more robust but I think the RGB channels are not in right order\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:,:,0] -= 129.1863\n",
    "        imarr[:,:,1] -= 104.7624\n",
    "        imarr[:,:,2] -= 93.5940        \n",
    "        aux = copy.copy(imarr)\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "    output= featmodel.predict(imarr)[0,:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inpDir,person,timestep,sample,ex_type=True): #access directory and extract face vector\n",
    "    \n",
    "    \n",
    "     \n",
    "    if ex_type == True:\n",
    "        Y_train_labels=np.ones((1,2622)) #*******\n",
    "    else:\n",
    "        Y_train_labels=np.zeros((1,2622))  #remove after training stage is completed  **********\n",
    "        \n",
    "     \n",
    "    sample_space=[] #for appending different utterances\n",
    "    feature_sequence = [] # for appending different timesteps\n",
    "     #input dataset most likely to be People_cerebrus/photos\n",
    "    linpDir = os.listdir(inpDir) #list all directories in dataset\n",
    "    #print(linpDir)\n",
    "    personStr= linpDir[person]\n",
    "    sampleFolder = '%s\\\\%s' % (inpDir,personStr) #opening sample folder\n",
    "    lsampleFolder = os.listdir(sampleFolder)\n",
    "    print(lsampleFolder)\n",
    "    sample= lsampleFolder[sample]\n",
    "    \n",
    "    utterFolder = '%s\\\\%s' % (sampleFolder,sample)\n",
    "    i = 0\n",
    "    lutterFolder = os.listdir(utterFolder)\n",
    "    #print(utterFolder)\n",
    "    #print(sample)\n",
    "    #print(lutterFolder)\n",
    "    \n",
    "    for utterances in lutterFolder:\n",
    "        print(utterances)\n",
    "        utterNumber= '%s\\\\%s' % (utterFolder,utterances)\n",
    "        lutterNumber= os.listdir(utterNumber)\n",
    "\n",
    "        frame = lutterNumber[timestep]\n",
    "        i = i + 1\n",
    "        image= \"%s\\\\%s\" % (utterNumber,frame) \n",
    "        im=Image.open(image)\n",
    "        im = im.resize((224,224))\n",
    "        feature_vector = features(trial_model,im, transform=True).reshape((1,1, 1, 2622))  #Dont change to 512\n",
    "        if i==1 :\n",
    "            feature_sequence=feature_vector #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            feature_sequence = np.concatenate((feature_sequence,feature_vector),axis=0)\n",
    "\n",
    "    #print(feature_sequence)\n",
    "    print(\"Extraction completed %d\"%i)\n",
    "    #print(WB_CNN.get_weights())\n",
    "    #CNN_out = WB_CNN.predict(feature_sequence)  \n",
    "    #print(CNN_out.shape)\n",
    "    print(\"Prediction done\")\n",
    "    \n",
    "    feature_sequence1 = feature_sequence[:,0,:,:]\n",
    "    return feature_sequence1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1 = feature_extractor('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Test\\\\photos',0,0,1,ex_type=True)\n",
    "#test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extractor(inpDir,person,sample,ex_type):\n",
    "    timesteps=20\n",
    "    count=0\n",
    "    for i in range(timesteps):\n",
    "        CNN_out=feature_extractor(inpDir,person,i,sample,ex_type)\n",
    "        count = count + 1\n",
    "        if count==1 :\n",
    "            CNN_out_total= CNN_out #maintaining same dimensions\n",
    "        else :\n",
    "            CNN_out_total= np.concatenate((CNN_out_total,CNN_out))\n",
    "        print(str(i) + 'timestep')\n",
    "        \n",
    "    print('Concatenation completed')\n",
    "    return CNN_out_total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_extractor(InpDir,person):\n",
    "    p_dataset=[]\n",
    "    count=0\n",
    "    for i in range(3): #*********************** Iterate over each word\n",
    "        count = count +1\n",
    "        if person==0 and i==0:\n",
    "            CNN_out_total= word_extractor(InpDir,person,i,True)\n",
    "        else:\n",
    "            CNN_out_total= word_extractor(InpDir,person,i,False)\n",
    "        if count==1 :\n",
    "            p_dataset = CNN_out_total #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            p_dataset = np.concatenate((p_dataset,CNN_out_total),axis=0)\n",
    "        print(str(i) + 'word')\n",
    "        print('Printing Dataset')\n",
    "        print(p_dataset)\n",
    "    print('----------------')\n",
    "    print(count)\n",
    "    print(p_dataset.shape)\n",
    "    return p_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = person_extractor('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Test\\\\photos',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(60):\n",
    "#    print(test[0] == test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_LSTM(sample_mat):\n",
    "    feature_vector=[]\n",
    "    feature_mat=[]\n",
    "    \n",
    "    count_word = 0\n",
    "    for word in range(3):\n",
    "        count_i=0\n",
    "        for utterance in range(1):\n",
    "            count_j=0\n",
    "            for timestep in range(20*word,20*(word+1),1):\n",
    "                timestep = timestep + utterance\n",
    "                sample_mat_j=sample_mat[timestep]\n",
    "                sample_mat_j=sample_mat_j.reshape((1,2622))\n",
    "                if count_j==0:\n",
    "                    feature_vector=sample_mat_j\n",
    "                else:\n",
    "                    feature_vector=np.concatenate((feature_vector,sample_mat_j))\n",
    "                count_j = count_j + 1\n",
    "\n",
    "            if count_i==0:\n",
    "                feature_mat=feature_vector\n",
    "            else:\n",
    "                feature_mat= np.concatenate((feature_mat,feature_vector))\n",
    "            count_i= count_i + 1\n",
    "            print(feature_mat.shape)\n",
    "            \n",
    "        if count_word==0:\n",
    "            LSTM_input=feature_mat\n",
    "        else:\n",
    "            LSTM_input= np.concatenate((LSTM_input,feature_mat))\n",
    "        count_word= count_word + 1\n",
    "                \n",
    "    print(LSTM_input.shape)\n",
    "  \n",
    "    return LSTM_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleNeck(InpDir):\n",
    "    p_dataset=person_extractor(InpDir,0)\n",
    "    p_dataset_LSTM=reshape_LSTM(p_dataset)\n",
    "    p_total_dataset=p_dataset_LSTM\n",
    "    print('person done')\n",
    "    return p_total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = BottleNeck('C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Test\\\\photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset1 = reshape_LSTM(dataset)\n",
    "#dataset2 = dataset1.reshape(3,20,2622)\n",
    "#dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2[0] == dataset2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
